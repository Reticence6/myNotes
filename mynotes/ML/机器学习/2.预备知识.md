# 数据操作与预处理

## 数据操作

### 课程位置

d2l-zh/pytorch/chapter-preliminaries/ndarray.ipynb

### Tensor 张量类

深度学习框架，张量类（在MXNet中为`ndarray`，在PyTorch和TensorFlow中为`Tensor`）都与Numpy的`ndarray`类似。但深度学习框架多了一些重要功能：首先，GPU很好地支持加速计算，而NumPy仅支持CPU计算； 其次，张量类支持自动微分。 这些功能使得张量类更适合深度学习。

深度学习存储和操作数据的主要接口是张量（𝑛n维数组）。它提供了各种功能，包括基本数学运算、广播、索引、切片、内存节省和转换其他Python对象

#### 赋值

引入torch包

```python
import torch
```

首先，可以使用 **arange** 创建一个行向量x。这个行向量包含以0开始的前12个整数，它们默认创建为整数。也可指定创建类型为浮点数。张量中的每个值都称为张量的元素(element)

```python
x = torch.arange(12)
x
tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
```

可以通过张量的**shape**属性来访问张量（沿每个轴的长度）的形状

```python
x.shape
torch.Size([12])
```

如果只想知道张量中元素的总数，即形状的所有元素乘积，可以检查它的大小（size）。因为这里在处理的是一个向量，所以它的shape与它的size相同

```python
x.numel()
12
```

**reshape**函数: 改变一个张量的形状而不改变元素数量和元素值（张量的形状改变，但元素值和张量的大小不会改变）

可以通过**-1**来调用此自动计算出维度的功能。即我们可以用x.reshape(-1,4)或x.reshape(3,-1)来取代x.reshape(3,4)

```python
X = x.reshape(3, 4)
X = tensor([[ 0,  1,  2,  3],
        	[ 4,  5,  6,  7],
        	[ 8,  9, 10, 11]])
```

[使用全0、全1、其他常量，或者从特定分布中随机采样的数字]来初始化矩阵

```python
torch.zeros((2, 3, 4))
torch.ones((2, 3, 4))
```

以下代码创建一个形状为（3,4）的张量。其中的每个元素都从均值为0、标准差为1的标准高斯分布（正态分布）中随机采样。

```python
torch.randn(3, 4)
tensor([[-0.0135,  0.0665,  0.0912,  0.3212],
        [ 1.4653,  0.1843, -1.6995, -0.3036],
        [ 1.7646,  1.0450,  0.2457, -0.7732]])
```

我们还可以[通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值]。 在这里，最外层的列表对应于轴0，内层的列表对应于轴1

```python
torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
```

#### 运算符

对于任意具有相同形状的张量， [**常见的标准算术运算符（`+`、`-`、`\*`、`/`和`\**`）都可以被升级为按元素运算**]。 我们可以在同一形状的任意两个张量上调用按元素操作。 

```python
x = torch.tensor([1.0, 2, 4, 8])
y = torch.tensor([2, 2, 2, 2])
x + y, x - y, x * y, x / y, x ** y  # **运算符是求幂运算
torch.exp(x)

(tensor([ 3.,  4.,  6., 10.]),
 tensor([-1.,  0.,  2.,  6.]),
 tensor([ 2.,  4.,  8., 16.]),
 tensor([0.5000, 1.0000, 2.0000, 4.0000]),
 tensor([ 1.,  4., 16., 64.]))
tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])
```

[**我们也可以把多个张量\*连结\*（concatenate）在一起**]， 把它们端对端地叠起来形成一个更大的张量。 我们只需要提供张量列表，并给出沿哪个轴连结。  

下面的例子分别演示了当我们沿行（轴-0，形状的第一个元素） 和按列（轴-1，形状的第二个元素）连结两个矩阵。  

 我们可以看到，第一个输出张量的轴-0长度（6）是两个输入张量轴-0长度的总和（3+3）； 第二个输出张量的轴-1长度（8）是两个输入张量轴-1长度的总和（4+4）。

```python
X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)

(tensor([[ 0.,  1.,  2.,  3.],
         [ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.],
         [ 2.,  1.,  4.,  3.],
         [ 1.,  2.,  3.,  4.],
         [ 4.,  3.,  2.,  1.]]),
 tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],
         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],
         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))
```

[**通过\*逻辑运算符\*构建二元张量**]。 以`X == Y`为例： 对于每个位置，如果`X`和`Y`在该位置相等，则新张量中相应项的值为1。

```python
X == Y

tensor([[False,  True, False,  True],
        [False, False, False, False],
        [False, False, False, False]])
```

**对张量中的所有元素进行求和，会产生一个单元素张量。**

```python
X.sum()

tensor(66.)
```

#### 广播机制

在上面的部分中，我们看到了如何在相同形状的两个张量上执行按元素操作。 在某些情况下，[**即使形状不同，我们仍然可以通过调用 \*广播机制\*（broadcasting mechanism）来执行按元素操作**]。 这种机制的工作方式如下：

1. 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；
2. 对生成的数组执行按元素操作。

例如：由于`a`和`b`分别是3×1和1×2矩阵，如果让它们相加，它们的形状不匹配。 我们将两个矩阵*广播*为一个更大的3×2矩阵，如下所示：矩阵`a`将复制列， 矩阵`b`将复制行，然后再按元素相加。

```python
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
a, b
(tensor([[0],
         [1],
         [2]]),
 tensor([[0, 1]]))
 
 a + b
tensor([[0, 1],
        [1, 2],
        [2, 3]])
```

#### 索引和切片

如下所示，我们[**可以用`[-1]`选择最后一个元素，可以用`[1:3]`选择第二个和第三个元素**]：

```python
X[-1], X[1:3]

(tensor([ 8.,  9., 10., 11.]),
 tensor([[ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.]]))
```

如果我们想[**为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值。**] 例如，`[0:2, :]`访问第1行和第2行，其中“:”代表沿轴1（列）的所有元素。 虽然我们讨论的是矩阵的索引，但这也适用于向量和超过2个维度的张量。

```python
X[0:2, :] = 12

tensor([[12., 12., 12., 12.],
        [12., 12., 12., 12.],
        [ 8.,  9., 10., 11.]])
```

#### 节省内存

[**运行一些操作可能会导致为新结果分配内存**]。 例如，如果我们用`Y = X + Y`，我们将取消引用`Y`指向的张量，而是指向新分配的内存处的张量。

在下面的例子中，我们用Python的`id()`函数演示了这一点， 它给我们提供了内存中引用对象的确切地址。 运行`Y = Y + X`后，我们会发现`id(Y)`指向另一个位置。 这是因为Python首先计算`Y + X`，为结果分配新的内存，然后使`Y`指向内存中的这个新位置。

```python
before = id(Y)
Y = Y + X
id(Y) == before

False
```

这可能是不可取的，原因有两个：

1. 首先，我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。通常情况下，我们希望原地执行这些更新；
2. 如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码可能会无意中引用旧的参数。

幸运的是，(**执行原地操作**)非常简单。 我们可以使用切片表示法将操作的结果分配给先前分配的数组，例如`Y[:] = <expression>`。 为了说明这一点，我们首先创建一个新的矩阵`Z`，其形状与另一个`Y`相同， 使用`zeros_like`来分配一个全0的块

```python
Z = torch.zeros_like(Y)
print('id(Z):', id(Z))
Z[:] = X + Y
print('id(Z):', id(Z))

id(Z): 140327634811696
id(Z): 140327634811696
```

如果在后续计算中没有重复使用`X`， 我们也可以使用`X[:] = X + Y`或`X += Y`来减少操作的内存开销。

```python
before = id(X)
X += Y
id(X) == before

True
```

#### 转换为其他Python对象

将深度学习框架定义的张量**转换为NumPy张量（`ndarray`）**很容易，反之也同样容易

```python
#  torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量。
A = X.numpy()
B = torch.tensor(A)
type(A), type(B)
A = X.numpy()
A = X.numpy()
B = torch.tensor(A)
type(A), type(B)

(numpy.ndarray, torch.Tensor)

# 要(将大小为1的张量转换为Python标量)，我们可以调用item函数或Python的内置函数。
a = torch.tensor([3.5])
a, a.item(), float(a), int(a)

(tensor([3.5000]), 3.5, 3.5, 3)
```



## 数据预处理

### 课程位置

d2l-zh/pytorch/chapter-preliminaries/pandas.ipynb

### 读取数据集

举一个例子，我们首先(**创建一个人工数据集，并存储在CSV（逗号分隔值）文件**) `../data/house_tiny.csv`中。 以其他格式存储的数据也可以通过类似的方式进行处理。 下面我们将数据集按行写入CSV文件中。

```python
import os

os.makedirs(os.path.join('..', 'data'), exist_ok=True)
data_file = os.path.join('..', 'data', 'house_tiny.csv')
with open(data_file, 'w') as f:
    f.write('NumRooms,Alley,Price\n')  # 列名
    f.write('NA,Pave,127500\n')  # 每行表示一个数据样本
    f.write('2,NA,106000\n')
    f.write('4,NA,178100\n')
    f.write('NA,NA,140000\n')
```

要[**从创建的CSV文件中加载原始数据集**]，我们导入`pandas`包并调用`read_csv`函数。该数据集有四行三列。其中每行描述了房间数量（“NumRooms”）、巷子类型（“Alley”）和房屋价格（“Price”）

```python
import pandas as pd
data = pd.read_csv(data_file)
print(data)

   NumRooms Alley   Price
0       NaN  Pave  127500
1       2.0   NaN  106000
2       4.0   NaN  178100
3       NaN   NaN  140000
```

### 处理缺失值

“NaN”项代表缺失值。 为了处理缺失的数据，典型的方法包括**插值法**和**删除法** 其中插值法用一个替代值弥补缺失值，而删除法则删除整行数据。

这里考虑插值：通过位置索引`iloc`，我们将`data`分成`inputs`和`outputs`， 其中前者为`data`的前两列，而后者为`data`的最后一列。 对于`inputs`中缺少的数值，我们用同一列的均值替换“NaN”项

```python
# iloc - index location
inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]
inputs = inputs.fillna(inputs.mean()) # 填均值
print(inputs)

   NumRooms Alley
0       3.0  Pave
1       2.0   NaN
2       4.0   NaN
3       3.0   NaN
```

[**对于`inputs`中的类别值或离散值，我们将“NaN”视为一个类别。**] 由于“巷子类型”（“Alley”）列只接受两种类型的类别值“Pave”和“NaN”， `pandas`可以自动将此列转换为两列“Alley_Pave”和“Alley_nan”。 巷子类型为“Pave”的行会将“Alley_Pave”的值设置为1，“Alley_nan”的值设置为0。 缺少巷子类型的行会将“Alley_Pave”和“Alley_nan”分别设置为0和1。

```python
inputs = pd.get_dummies(inputs, dummy_na=True)
print(inputs)

   NumRooms  Alley_Pave  Alley_nan
0       3.0           1          0
1       2.0           0          1
2       4.0           0          1
3       3.0           0          1
```

### 转换为张量

[**现在`inputs`和`outputs`中的所有条目都是数值类型，它们可以转换为张量格式。**] 当数据采用张量格式后，可以通过在 :numref:`sec_ndarray`中引入的那些张量函数来进一步操作。

```python
import torch

X = torch.tensor(inputs.to_numpy(dtype=float))
y = torch.tensor(outputs.to_numpy(dtype=float))
X, y

(tensor([[3., 1., 0.],
         [2., 0., 1.],
         [4., 0., 1.],
         [3., 0., 1.]], dtype=torch.float64),
 tensor([127500., 106000., 178100., 140000.], dtype=torch.float64))
# 传统python浮点数会使用64位浮点数，但是计算较慢，对于深度学习来讲，一般使用32位浮点数
```

### tensor与array的区别

tensor-张量，是数学上的一个概念

array-数组，是计算机的一个概念

用起来其实是一个东西



# 线性代数

## 课程位置

d2l-zh/pytorch/chapter-preliminaries/linear-algebra.ipynb

## 张量

[**就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构**]。 张量（本小节中的“张量”指代数对象）是描述具有任意数量轴的𝑛维数组的通用方法。 例如，向量是一阶张量，矩阵是二阶张量。 张量用特殊字体的大写字母表示（例如，𝖷、𝖸和𝖹）， 它们的索引机制（例如𝑥 𝑖𝑗𝑘和[𝖷]1,2𝑖−1,3）与矩阵类似。

当我们开始处理图像时，张量将变得更加重要，图像以𝑛维数组形式出现， 其中3个轴对应于高度、宽度，以及一个*通道*（channel）轴， 用于表示颜色通道（红色、绿色和蓝色）。 现在先将高阶张量暂放一边，而是专注学习其基础知识。

```python
X = torch.arange(24).reshape(2, 3, 4)
X

tensor([[[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]],

        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]]])
```

## 降维

:label:`subseq_lin-alg-reduction`

我们可以对任意张量进行的一个有用的操作是[**计算其元素的和**]。 数学表示法使用∑符号表示求和。 为了表示长度为𝑑的向量中元素的总和，可以记为∑𝑑 𝑖=1 𝑥𝑖。 在代码中可以调用计算求和的函数：

```python
x = torch.arange(4, dtype=torch.float32)
x, x.sum()

(tensor([0., 1., 2., 3.]), tensor(6.))
```

我们可以(**表示任意形状张量的元素和**)。 例如，矩阵𝐀中元素的和可以记为：
$$
\sum_{i=1}^{m} \sum_{j=1}^{n} a_{ij}
$$

```python
A.shape, A.sum()

(torch.Size([5, 4]), tensor(190.))
```

默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。 我们还可以[**指定张量沿哪一个轴来通过求和降低维度**]。 以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定`axis=0`。 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。

```python
A_sum_axis0 = A.sum(axis=0)
A_sum_axis0, A_sum_axis0.shape

(tensor([40., 45., 50., 55.]), torch.Size([4]))
```

指定`axis=1`将通过汇总所有列的元素降维（轴1）。因此，输入轴1的维数在输出形状中消失。

```python
A_sum_axis1 = A.sum(axis=1)
A_sum_axis1, A_sum_axis1.shape

(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))
```

沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和。

```python
A.sum(axis=[0, 1])  # 结果和A.sum()相同

tensor(190.)
```

[**一个与求和相关的量是\*平均值\*（mean或average）**]。 我们通过将总和除以元素总数来计算平均值。 在代码中，我们可以调用函数来计算任意形状张量的平均值。

```python
A.mean(), A.sum() / A.numel()

(tensor(9.5000), tensor(9.5000))
```

同样，计算平均值的函数也可以沿指定轴降低张量的维度。

```python
A.mean(axis=0), A.sum(axis=0) / A.shape[0]

(tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))
```

## 非降维求和

:label:`subseq_lin-alg-non-reduction`

但是，有时在调用函数来[**计算总和或均值时保持轴数不变**]会很有用。

```python
sum_A = A.sum(axis=1, keepdims=True)
sum_A

tensor([[ 6.],
        [22.],
        [38.],
        [54.],
        [70.]])
```

例如，由于`sum_A`在对每行进行求和后仍保持两个轴，我们可以(**通过广播将`A`除以`sum_A`**)。

```python
A / sum_A

tensor([[0.0000, 0.1667, 0.3333, 0.5000],
        [0.1818, 0.2273, 0.2727, 0.3182],
        [0.2105, 0.2368, 0.2632, 0.2895],
        [0.2222, 0.2407, 0.2593, 0.2778],
        [0.2286, 0.2429, 0.2571, 0.2714]])
```

如果我们想沿[**某个轴计算`A`元素的累积总和**]， 比如`axis=0`（按行计算），可以调用`cumsum`函数。 此函数不会沿任何轴降低输入张量的维度。

```python
A.cumsum(axis=0)

tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  6.,  8., 10.],
        [12., 15., 18., 21.],
        [24., 28., 32., 36.],
        [40., 45., 50., 55.]])
```

# 矩阵计算

## 求导

1. 求导图

![image-20250304104219320](images\2.1矩阵计算.png)

2. 标量求导向量

![image-20250304104448838](images\2.2标量向量求导.png)

3. 向量求导向量

![image-20250304104359340](images\2.3向量求导.png)

## 求导-链式法则

![image-20250304171652275](images\2.4链式法则例子.png)

## 自动求导

### 课程位置

d2l-zh/pytorch/chapter-preliminaries/linear-algebra.ipynb

### 自动求导介绍

1. 自动求导：计算一个函数在指定值上的导数

2. 有别于符号求导和数值求导

3. 计算图

   - 将代码分解成操作子
   - 将计算表示成一个有向无环图
   - 显示构造
     - Tensorflow/Theano/MXNet
   - 隐式构造
     - PyTorch/MXNet

4. 自动求导的两种模式

   - 自动求导的两种模式

     <img src="images\2.5 自动求导的两种模式.png" alt="image-20250304172802521" style="zoom: 80%;" />

   - 反向累积

     ![image-20250304173013003](images\2.6反向累积.png)

     <img src="images\2.7 反向累积总结.png" alt="image-20250304173243420" style="zoom:67%;" />

   - 复杂度对比

     > 计算复杂度：O(n), n是操作子个数，通常正向和反向的代价类似
     >
     > 内存复杂度：反向-O(n)，因为需要存储正向的所有中间结果
     >
     > 跟正向累积对比：
     >
     > ​     O(n)计算复杂度用来计算一个变量的梯度
     >
     > ​     O(1)内存复杂度

### 自动求导的实现

听不懂

loss一般都是标量，所以深度学习中一般对标量求导

深度学习中，梯度需要先正向求再反向求

例：对 y=2xTx 关于列向量x求导

```python
import torch
# 创建变量x
x = torch.arange(4.0)
# 设置存储梯度
x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)
x.grad  # 默认值是None
# 计算y  dot-点积
y = 2 * torch.dot(x, x)
# 利用反向传播函数计算梯度，再打印
y.backward()
x.grad

Out: tensor([ 0.,  4.,  8., 12.])

# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值
x.grad.zero_()
```

