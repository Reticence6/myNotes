# 线性回归

## 线性回归组成

### 线性模型

线性假设是指目标可以表示为特征的加权和

如：w-权重，b-偏置值/偏移量/截距，y-预测值，x-特征值。
$$
\mathrm{y} = w_{\mathrm{1}} \cdot \mathrm{x_{\mathrm{1}}} + w_{\mathrm{2}} \cdot \mathrm{x_{\mathrm{2}}} + ...+ w_{\mathrm{n}} \cdot \mathrm{x_{\mathrm{n}}} + b.
$$
将所有特征放到向量x∈Rd中， 并将所有权重放到向量w∈Rd中， 我们可以用点积形式来简洁地表达模型：
$$
\hat{y} = \mathbf{w}^\top \mathbf{x} + b.
$$
对于特征集合X，预测值y^∈Rn 可以通过矩阵-向量乘法表示为：
$$
{\hat{\mathbf{y}}} = \mathbf{X} \mathbf{w} + b
$$
线性模型可以看作是单层神经网络——输入层有n个数据，输出层有一个

### 损失函数

*损失函数*（loss function）能够量化目标的*实际*值与*预测*值之间的差距

回归问题中最常用的损失函数是平方误差函数：（平方损失）

（1/2是为了求导消去2）
$$
l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2.
$$

### 训练数据

收集数据点来决定参数值，通常越多越好

### 参数学习

- 训练损失
  $$
  L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.
  $$
  
- 最小化损失来学习函数
  $$
  \mathbf{w}^*, b^* = \operatorname*{argmin}_{\mathbf{w}, b}\  L(\mathbf{w}, b).
  $$

### 显示解

线性回归问题具有显示解

（一系列过程我也看不懂，反正是说损失是凸函数，则最优解满足梯度为0）
$$
\mathbf{w}^* = (\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top \mathbf{y}.
$$

## 基础优化方法

### 梯度下降

即使在我们无法得到解析解的情况下，我们仍然可以有效地训练模型。 在许多任务上，那些难以优化的模型效果要更好。 因此，弄清楚如何训练这些难以优化的模型是非常重要的

**梯度下降**（gradient descent）的方法， 这种方法几乎可以优化所有深度学习模型。 它通过不断地在损失函数递减的方向上更新参数来降低误差。

梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值） 关于模型参数的导数（在这里也可以称为梯度）。但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。 因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本， 这种变体叫做**小批量随机梯度下降**

在每次迭代中，我们首先随机抽样一个小批量B， 它是由固定数量的训练样本组成的。 然后，我们计算小批量的平均损失关于模型参数的导数（也可以称为梯度）。最后，我们将梯度乘以一个预先确定的正数η，并从当前参数的值中减掉。
$$
(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).
$$
η-学习率：步长的超参数

b-批量大小：随机采样的样本数，另一个重要的超参数

> 注意：
>
> 梯度下降通过不断沿着反梯度方向更新参数求解
>
> （梯度-代表函数值改变最快的方向）
>
> 小批量随机梯度下降是深度学习默认的求解算法
>
> 两个重要的超参数：批量大小和学习率

## 线性回归的实现

### 从零实现线性回归

创建数据集，选取样本，定义模型参数，定义模型，定义损失函数，定义优化算法(随机梯度下降)，训练

```python
# 从零开始实现 线性回归

# 随机梯度下降、随机初始化 需要随机
import random
import torch
from d2l import torch as d2l

# 首先-根据带噪声的线性模型构造一个人造数据集
# 假设真实的模型，w=[2,-3.4]T  b=4.2   y=Xw+b+θ
def synthetic_data(w, b, num_examples):
    """生成 y = Xw + b + 噪声"""
    # x-均值为0、方差为1的一个随机数，大小为num_examples、列数为w的长度
    X = torch.normal(0, 1, (num_examples, len(w)))
    # 生成y
    y = torch.matmul(X, w) + b
    # 加一个随机噪声
    y += torch.normal(0, 0.01, y.shape)
    # X, y作为一个列向量返回
    return X, y.reshape((-1, 1))

# 根据真实的w和b生成特征和标注
# true_w = torch.tensor([2, -3.4])
# true_b = 4.2
# features, labels = synthetic_data(true_w, true_b, 1000)

# 第0个样本的特征-长为2的向量
# print('features: ', features[0], '\nlabel:', labels[0])


# d2l.set_figsize()
# 画出特征的第一列
# detach-pytorch的某些版本需要把他从计算图里分离出来(分离出数值，不再含有梯度)，才能转到numpy处理
# d2l.plt.scatter(features[:, 1].detach().numpy(),
#                 labels.detach().numpy(), 1)
# d2l.plt.show()


# 每次选取b个样本返回
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    # 生成每个样本的index, range-0~n-1, 然后转成一个list
    indices = list(range(num_examples))
    # 将序号随机打乱 -> 样本是随机读取的, 没有特定的顺序
    random.shuffle(indices)
    # 从0到n-1，每次跳batch_size个大小，生成一个批次的数据
    for i in range(0, num_examples, batch_size):
        # 拿b个样本
        # 如果最后一个批量的样本数不足batch_size，则通过min确保不会超出数据集的范围。
        batch_indices = torch.tensor(indices[i: min(i + batch_size, num_examples)])
        # 返回对应的特征和标签
        # yield相当于return返回一个值，并且记住返回的位置，下次迭代时，从yield的下一条语句开始执行
        yield features[batch_indices], labels[batch_indices]

# batch_size = 10
#
# for X,y in data_iter(batch_size, features, labels):
#     print(X, '\n', y)
#     break

# # 定义 初始化模型参数
# # 输入维度为2-w长为2的一个向量。随机初始化为均值为0、方差为0.01的正态分布。需要计算梯度
# w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)
# # 标量
# b = torch.zeros(1, requires_grad=True)

# 定义模型
def linreg(X, w, b):
    """"线性回归模型"""
    # 矩阵X 乘 向量w 再加 b
    # 返回预测值
    return torch.matmul(X, w) + b

# 定义损失函数
def squared_loss(y_hat, y):
    """均方损失"""
    # (预测值 - 真实值)^2 / 2
    return (y_hat - y.reshape(y_hat.shape))**2 / 2

# 定义优化算法
# params-所有参数w、b，lr-学习率，batch_size-批量大小
def sgd(params, lr, batch_size):
    """"小批量随机梯度下降"""
    # 更新的时候不需要参与梯度计算
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()


# 准备数据集
true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000)

# 定义 初始化模型参数
w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)

# 训练过程
lr = 0.03
batch_size = 10
num_epochs = 3
net = linreg
loss = squared_loss

# 每次扫一遍数据
for epoch in range(num_epochs):
    # 每次拿出一个批量的X和y
    for X,y in data_iter(batch_size, features, labels):
        # net做预测，和真实y做损失，l就是一个长为批量大小的向量
        l = loss(net(X, w, b), y)
        # 求和后算梯度(求和是因为矩阵无法算梯度，转化为标量可以计算)
        l.sum().backward()
        # 利用sgd对w和b进行更新
        # 真实情况可能最后一个批量会少一点元素
        sgd([w, b], lr, batch_size)
    # 评价进度
    # 不需要计算梯度
    with torch.no_grad():
        # 将所有数据传进去，计算损失
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')

# 比较真实参数和通过训练学到的参数来评估训练的成功程度
print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')
print(f'b的估计误差: {true_b - b}')


Output:
    epoch 1, loss 2.188373
    epoch 2, loss 0.290504
    epoch 3, loss 0.039141
    w的估计误差: tensor([ 0.1290, -0.1296], grad_fn=<SubBackward0>)
    b的估计误差: tensor([0.2139], grad_fn=<RsubBackward1>)
```

超参数的选择

不同的值得到的结果会有不同

### 线性回归的简洁实现

使用了pytorch中的nn模块实现神经网络

线性回归属于只有一层，有多个输入、一个输出的神经网络

```python
# 线性回归的简洁实现：使用pytorch的nn提供的数据预处理的模块来实现的更加简单
import numpy as np
import torch
from torch.utils import data
from d2l import torch as d2l
# nn是神经网络的缩写
from torch import nn

# 构造特征值和标签
true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = d2l.synthetic_data(true_w, true_b, 1000)

#
def load_array(data_arrays, batch_size, is_train=True):
    """构造一个PyTorch数据迭代器"""
    dataset = data.TensorDataset(*data_arrays)
    # dataloader-每次从数据集中随机挑选b个样本出来，shuffle-是不是要随机取
    return data.DataLoader(dataset, batch_size, shuffle=is_train)

batch_size = 10
data_iter = load_array((features, labels),batch_size)
next(iter(data_iter))

# 模型的定义, 输入维度为2、输出维度为1
# 线性回归只有一层，但是之后有复杂的多层，所以需要放在Sequential的容器中(List of layers 按层放)
net = nn.Sequential(nn.Linear(2, 1))

# 初始化模型参数 w、b
# normal_(0, 0.01) - 使用正态分布替换掉data的值
net[0].weight.data.normal_(0, 0.01)
# bias - 偏差
net[0].bias.data.fill_(0)

# 定义误差函数 -- 均方误差 MSELoss类
loss = nn.MSELoss()

# 实例化SGD实例
# SGD(拿出所有的参数, 学习率)
trainer = torch.optim.SGD(net.parameters(), lr=0.03)

# 训练过程
num_epoch = 3
for epoch in range(num_epoch):
    for X, y in data_iter:
        # net(X)-预测  loss
        l = loss(net(X), y)
        # 先把梯度清零
        trainer.zero_grad()
        # 计算backward，pytorch已经做了sum，所以不需要单独再做
        l.backward()
        # 进行模型的更新
        trainer.step()
    # 扫过一遍数据之后，把整个的数据测试一次
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')
    
    
Output:
    epoch 1, loss 0.000356
	epoch 2, loss 0.000103
	epoch 3, loss 0.000103
```

### 一些问题

- 损失为什么要求平均

- 怎么找到合适的学习率

  > 找一个对学习率不那么敏感的方法
  >
  > 合理的参数初始化，学习率大概设置个0.001就差不多了

- batch_size的设置

  > batch_size越小，对收敛越好




# Softmax回归

## 介绍

（Softmax回归-虽然叫回归，其实是一个分类问题）

### 回归vs分类

回归：估计一个连续值，如房价

- 单连续数值输出
- 自然区间R
- 跟真实值的区别作为损失

分类：预测一个离散的类别，如区分图片里是猫还是狗

- 通常有多个输出
- 输出i是预测为第i类的置信度

> 如：MNIST-手写数字实别(10类)、ImageNet-自然物体分类(1000类)

从回归到多类分类

- 均方损失

  > 对类别进行一位编码  y = [y1, y2, ...] T
  >
  > yi = 1 if i=y; 0 otherwise (是该类别就赋1，其余为0)
  >
  > 使用均方损失训练
  >
  > 最大值最为预测 y^ = argmax Oi

- 无校验比例

  > 正确类别的置信度能不能更大，远远大于非正确类别的Oi
  >
  > （关心的是一个相对值）

- 校验比例

  > 输出匹配概率(非负且和为1)
  >
  > 将预测值各项转化为占整体的概率
  > $$
  > \hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{其中}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}
  > $$

- Softmax和交叉熵损失

  > 交叉熵常用来衡量两个概率的区别，将它作为损失
  >
  > (y其实只有正确类别的那个值为1，所以交叉熵其实是预测中正确类别的概率的log加负数)
  > $$
  > l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j = - \log \hat{y}_y.
  > $$
  > (交叉熵与非正确类别的值无关)
  >
  > 其梯度是真实概率和预测概率的区别
  > $$
  > \partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j.
  > $$

### 损失函数

三个常用损失函数

1. 均方损失
2. 绝对值损失函数
3. Huber‘s Robust Loss -> 1和2的改进

### 读取图像分类数据集

1. 图像分类数据集

   > MNIST是图像分类中广泛使用的数据集之一，但作为基准数据集过于简单
   >
   > 所以使用类似但更复杂的Fashion-MNIST数据集

2. 读取数据集

   - 下载数据集

   ```python
   import torch
   # pytorch实现计算机视觉的库
   import torchvision
   # 方便读取数据
   from torch.utils import data
   # 操作数据
   from torchvision import transforms
   from d2l import torch as d2l
   
   # 用svg显示图片，清晰度高一点
   d2l.use_svg_display()
   
   # 通过框架中的内置函数将Fashion-MNIST数据集下载并读取到内存
   # 通过 ToTenser实例 将图像数据从PIL类型变换成32位浮点数格式
   # 并除以255使得所有像素的数值均在0到1之间
   
   # 图片转化为pytorch的tensor，做最简单的预处理
   trans = transforms.ToTensor()
   # 下载到data中，下载的是训练数据集，trans-拿出来之后得到tensor，默认从网上下载
   mnist_train = torchvision.datasets.FashionMNIST(root="../data", train=True, transform=trans, download=True)
   mnist_test = torchvision.datasets.FashionMNIST(root="../data", train=False, transform=trans, download=True)
   
   # 输出图片个数
   print(len(mnist_train), len(mnist_test))
   # 第一张图片的形状
   print(mnist_train[0][0].shape)
   
   Output：
   60000 10000
   torch.Size([1, 28, 28])  --> RGB的Channel为1(为黑白图片)、长宽都为28
   ```

   - 可视化数据集

   ```python
   def get_fashion_mnist_labels(labels):  #@save
       """返回Fashion-MNIST数据集的文本标签"""
       text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
                      'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
       return [text_labels[int(i)] for i in labels]
   
   def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  #@save
       """绘制图像列表"""
       figsize = (num_cols * scale, num_rows * scale)
       _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)
       axes = axes.flatten()
       for i, (ax, img) in enumerate(zip(axes, imgs)):
           if torch.is_tensor(img):
               # 图片张量
               ax.imshow(img.numpy())
           else:
               # PIL图片
               ax.imshow(img)
           ax.axes.get_xaxis().set_visible(False)
           ax.axes.get_yaxis().set_visible(False)
           if titles:
               ax.set_title(titles[i])
       return axes
   
   # 把数据放到dataloader中，批量大小固定为18，next-拿到第一个小批量，得到X、y
   X, y = next(iter(data.DataLoader(mnist_train, batch_size=18)))
   # 把Xreshape为(18,28,28)，画2行、每行9张图片，title拿出对应标号
   show_images(
       X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y))
   # 记得加这句
   d2l.plt.show()
   ```

   - 读取以小批量数据，大小为batch_size

   ```python
   batch_size = 256
   
   # 正常训练数据存放在硬盘中，根据CPU的性能选择不同数量的进程读取数据
   def get_dataloader_workers():  #@save
       """使用4个进程来读取数据"""
       return 4
   
   # 数据集、批量、是否随机、几个进程
   train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True,
                                num_workers=get_dataloader_workers())
   
   # 测试速度
   timer = d2l.Timer()
   # 访问所有batch
   for X, y in train_iter:
       continue
   # 读取一次数据的时间
   print(f'{timer.stop():.2f} sec')
   
   ```

   通常在训练前，会看数据读取的时间有多快，一般读取数据要比训练时间快很多

3. 整合起来，定义了`load_data_fashion_mnist`函数

   ```python
   # 定义load_data_fashion_mnist函数，用于获取和读取Fashion-MNIST数据集。 这个函数返回训练集和验证集的数据迭代器。 此外，这个函数还接受一个可选参数resize，用来将图像大小调整为另一种形状。
   def load_data_fashion_mnist(batch_size, resize=None):  #@save
       """下载Fashion-MNIST数据集，然后将其加载到内存中"""
       trans = [transforms.ToTensor()]
       if resize:
           trans.insert(0, transforms.Resize(resize))
       trans = transforms.Compose(trans)
       mnist_train = torchvision.datasets.FashionMNIST(
           root="../data", train=True, transform=trans, download=True)
       mnist_test = torchvision.datasets.FashionMNIST(
           root="../data", train=False, transform=trans, download=True)
       return (data.DataLoader(mnist_train, batch_size, shuffle=True,
                               num_workers=get_dataloader_workers()),
               data.DataLoader(mnist_test, batch_size, shuffle=False,
                               num_workers=get_dataloader_workers()))
       
       
   train_iter, test_iter = load_data_fashion_mnist(32, resize=64)
   for X, y in train_iter:
       print(X.shape, X.dtype, y.shape, y.dtype)
       break
   ```

   

## 从零开始实现

1. 准备数据

   ```python
   """"准备环节"""
   batch_size = 256
   # 训练集和测试集的迭代器
   # 修改了它的代码，把进程由4改为了2(电脑带不动)
   train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
   
   # 图片是一个3D的输入(28,28,1)，而Softmax需要的输入是一个向量，所以需要把图片拉成一个向量，但是会损失空间信息(之后卷积讲)
   # 展平每个图像，将它们视为长度为784的向量
   # 因为数据集有10个类别，所以网络输出维度位10
   num_inputs = 784
   num_outputs = 10
   # W权重-高斯随机分布的值(均值为0、方差为0.01)，形状-行数为输入的个数、列数为输出的个数
   W = torch.normal(0, 0.01, size =(num_inputs, num_outputs), requires_grad=True)
   # b偏移-每个输出都有一个偏移
   b = torch.zeros(num_outputs, requires_grad=True)
   ```

2. 定义Softmax操作

   ```python
   # X成了一个矩阵，要对每一行做Softmax(求每个点占他这一行的比例)
   def softmax(X):
       # 保证非负数
       X_exp = torch.exp(X)
       # 除数
       partition = X_exp.sum(1, keepdim=True)
       return X_exp / partition # 这里使用了广播机制
   ```

3. 实现Softmax回归模型

   ```python
   def net(X):
       # X-reshape成256*784的向量
       # X.shape=256*784 W.shape=784*10, b.shape=1*10，传入X*W+b
       return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)
   ```

4. 实现交叉熵损失函数

   ```python
   def cross_entropy(y_hat, y):
       # range(len(y_hat))- 0~y_hat长度的向量
       # y_hat[range(len(y_hat)), y] 根据预测值，拿出对应位置的真实值，然后求-log
       return -torch.log(y_hat[range(len(y_hat)), y])
   ```

5. 计算预测正确的数量

   ```python
   def accuracy(y_hat, y):
       if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
           # 预测值中每行元素值最大的下标
           y_hat = y_hat.argmax(axis=1)
       # 把y_hat转为y的数据类型，然后和y比较，得到一个bool类型
       cmp = y_hat.type(y.dtype) == y
       # 再转成和y一样的形状，求和，转为一个浮点数
       return float(cmp.type(y.dtype).sum())
   ```

6. 计算在指定数据集上模型的精度

   ```python
   def evaluate_accuracy(net, data_iter):
       if isinstance(net, torch.nn.Module):
           # 将模型设置为评估模式
           net.eval()
       # 正确预测数、预测总数
       metric = d2l.Accumulator(2)
       for X, y in data_iter:
           # accuracy(net(X), y) - X放到net中得到预测值，再与y计算得到预测正确的样本数
           # y.numel() - 样本总数
           # 放进Accumulator这样的迭代器中-累加器
           metric.add(accuracy(net(X), y), y.numel())
       # 返回 正确的样本数 / 总样本数
       return metric[0] / metric[1]
   ```

7. 迭代一次的训练

   ```python
   def train_epoch_ch3(net, train_iter, loss, updater):
       # 如果用nn模型，开启训练模式
       if isinstance(net, torch.nn.Module):
           net.train()
       # 使用长度为3的累加器
       metric = d2l.Accumulator(3)
       for X, y in train_iter:
           y_hat = net(X)
           # 使用交叉熵计算损失
           l = loss(y_hat, y)
           # 使用PyTorch内置的优化器和损失函数
           if isinstance(updater, torch.optim.Optimizer):
               updater.zero_grad()
               l.backward()
               updater.step()
               metric.add(
                   float(l) * len(y), accuracy(y_hat, y),
                   y.size().numel())
           # 使用定制的优化器和损失函数
           else:
               l.sum().backward()
               updater(X.shape[0])
               metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
       # 返回训练损失和训练精度
       return metric[0] / metric[2], metric[1] / metric[2]
   ```

8. 训练

   ```python
   def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):
       plt.ion()  # 开启交互模式
       animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
                           legend=['train loss', 'train acc', 'test acc'])
       for epoch in range(num_epochs):
           # 训练，更新模型，返回训练误差
           train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
           # 在测试数据集上测试精度
           test_acc = evaluate_accuracy(net, test_iter)
           # 动画显示
           animator.add(epoch + 1, train_metrics + (test_acc,))
       plt.ioff()  # 关闭交互模式
       d2l.plt.show()
       train_loss, train_acc = train_metrics
   ```

9. 执行训练

   ```python
   """小批量随机梯度下降来优化模型的损失函数"""
   lr = 0.1
   def updater(batch_size):
       return d2l.sgd([W, b], lr, batch_size)
   
   # 开始训练
   num_epochs = 10
   train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)
   ```

10. 预测

    ```python
    def predict_ch3(net, test_iter, n=6):
        # 测试数据集中拿出一个样本
        for X, y in test_iter:
            break
        # 真实标号
        trues = d2l.get_fashion_mnist_labels(y)
        # 预测标号
        preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))
        titles = [true + '\n' + pred for true, pred in zip(trues, preds)]
        d2l.show_images(
            X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])
        d2l.plt.show()
    
    predict_ch3(net, test_iter)
    ```

## 简洁实现

通过深度学习框架的高级API——pytorch的nn来实现Softmax回归

```python
batch_size = 256
# 训练集和测试集的迭代器
# 修改了它的代码，把进程由4改为了2(电脑带不动)
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

# Softmax不会显示调整输入的形状
# 因此定义了展平层(flatten)在线性层前调整网络输入的形状
# flatten-把任何维度的tensor变成一个2D的tensor
# 输入是256*28*28, Flatten将28*28展平了, 所以输入变成了256*784
net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))

# 初始化权重
# m-当前的layer
def init_weights(m):
    # 如果是线性层，则把权重初始化为一个随机值(均值为0, 方差为0.01)
     if type(m) == nn.Linear:
         nn.init.normal_(m.weight, std=0.01)

# 每层都初始化
net.apply(init_weights)

# 交叉熵损失函数，传递未归一化的预测，同时计算Softmax及其对数
loss = nn.CrossEntropyLoss()

# 使用学习率为0.1的小批量随机梯度下降作为优化算法
trainer = torch.optim.SGD(net.parameters(), lr=0.1)

# 调用之前定义的训练函数来训练模型
num_epochs = 10
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

## 问题

- Softlabel
